<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Section 1 - PP's Notes</title>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../css/highlight.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Section 1";
    var mkdocs_page_input_path = "DeepLearning/section_1.md";
    var mkdocs_page_url = "/DeepLearning/section_1/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js"></script>
  <script src="../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> PP's Notes</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../..">Welcome</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">User profile</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../UserProfile/Introduction/">Introduction</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Classification</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../Classification/logistic_regression/">Logistic Regression</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Classification/random_forest/">Random Forest</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Classification/support_vector_machine/">Support Vector Machine(SVM)</a>
                </li>
                <li class="">
                    
    <a class="" href="../../Classification/back_propagation_neural_networks/">Multi-Layer Perceptron(MLP)</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Deep Learning</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Introduction/">Introduction</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">Section 1</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#_1">第二章 线性代数</a></li>
    

    <li class="toctree-l3"><a href="#_2">第三章 概率与信息论</a></li>
    

    <li class="toctree-l3"><a href="#_3">第四章 数值计算</a></li>
    

    <li class="toctree-l3"><a href="#_4">第五章</a></li>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../section_2/">Section 2</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">剑指offer</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../code_offer/Introduction/">Introduction</a>
                </li>
                <li class="">
                    
    <a class="" href="../../code_offer/java1-10/">Java 1-10</a>
                </li>
                <li class="">
                    
    <a class="" href="../../code_offer/java11-20/">Java 11-20</a>
                </li>
                <li class="">
                    
    <a class="" href="../../code_offer/java21-30/">Java 21-30</a>
                </li>
                <li class="">
                    
    <a class="" href="../../code_offer/java31-40/">Java 31-40</a>
                </li>
                <li class="">
                    
    <a class="" href="../../code_offer/java41-50/">Java 41-50</a>
                </li>
                <li class="">
                    
    <a class="" href="../../code_offer/java51-60/">Java 51-60</a>
                </li>
                <li class="">
                    
    <a class="" href="../../code_offer/java61-66/">Java 61-66</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">PP's Notes</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Deep Learning &raquo;</li>
        
      
    
    <li>Section 1</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="_1">第二章 线性代数</h1>
<p>没啥要写的。</p>
<h1 id="_2">第三章 概率与信息论</h1>
<ul>
<li>3.3 概率分布</li>
</ul>
<p>离散型随机变量的概率分布用<code>概率质量函数</code>(probability mass function, PMF)来表述。用P表示概率质量函数。</p>
<p>连续型随机变量的概率分布用<code>概率密度函数</code>(probability density function, PDF)来描述。用p表示。</p>
<h1 id="_3">第四章 数值计算</h1>
<ul>
<li>4.1 上溢和下溢</li>
</ul>
<p>必须对上溢和下溢进行数值稳定的例子是softmax函数。<code>softmax函数</code>经常用语预测与Multinoulli分布相关联的概率。</p>
<p>
<script type="math/tex; mode=display">softmax({\boldsymbol x})_i=\frac{exp(x_i)}{\sum_{j=1}^{n}exp(x_j)}</script>
</p>
<p>在某些情况下，我们可能在实现一个新的算法是自动保持数值的稳定。比如Theano软件包，能自动检测病稳定深度学习中许多常见的数值不稳定的表达式。</p>
<ul>
<li>4.2 病态条件</li>
</ul>
<p>输入被轻微扰动而迅速改变的函数对科学计算来说可能是有问题的，因为输入中的摄入误差可能导致输出的巨大变化。</p>
<p>比如函数$f({\boldsymbol x})={\boldsymbol A}^{-1}{\boldsymbol x}$如果矩阵${\boldsymbol A}$的<code>最大特征值和最小特征值的模之比(绝对值)</code>很大时，矩阵求逆对输入的误差特别敏感。</p>
<ul>
<li>4.3 基于梯度的优化方法。</li>
</ul>
<p>梯度(gradient)是相对一个向量求导的导数，f的导数是包含所有偏导数的向量，记为$\nabla_xf({\boldsymbol x})$。在多维的情况下，临界点是梯度中所有元素都为0的点。</p>
<p>在负梯度的方向上移动可以减小f。这是最速下降法(method of steepest descent)或<code>梯度下降</code>(gradient descent)。$\epsilon$为学习率，<code>普遍的方式是选择一个小常数</code>。还有一种方法是分局几个$\epsilon$计算$f({\boldsymbol x}-\epsilon\nabla_xf({\boldsymbol x}))$，并选择其中能产生最小目标函数值的$\epsilon$。这种策略为<code>线搜索</code>。</p>
<p>
<script type="math/tex; mode=display">{\boldsymbol x}'={\boldsymbol x}-\epsilon\nabla_xf({\boldsymbol x})</script>
</p>
<p>虽然梯度下降被限制在连续空间中的优化问题，但可以推广到<code>离散空间</code>。递增带有离散参数的目标函数称为<code>爬山</code>(hill climbing)算法。</p>
<ul>
<li>4.3.1 梯度之上：Jacobian和Hessian矩阵</li>
</ul>
<p>有时我们需要计算输入和输出都是向量的函数的所有偏导数。包含所有这样的偏导数的矩阵被称为雅可比(Jacobian)矩阵.</p>
<p>有时我们对二阶导数(曲率)感兴趣，我们将这些导数合并为一个矩阵，称为黑赛(Hessian)矩阵。Hessain矩阵等价于梯度的Jacobian矩阵。在深度学习背景下，遇到的大多数函数的Hessian矩阵几乎处处都是对称的。</p>
<p>因为Hessain矩阵是实对称的，我们可以将其分解为一组实特征值和一组特征向量的正交基。在特定方向${\boldsymbol d}$上的二阶导数可以写成${\boldsymbol d}^T\boldsymbol{Hd}$（${\boldsymbol d}$是一个特征向量，这个式子的乘积是一个数，是与特征向量${\boldsymbol d}$对应的特征值）。当${\boldsymbol d}$是${\boldsymbol H}$的一个特征向量时，<code>这个方向的二阶导数就是对应的特征值</code>。对于其他方向的${\boldsymbol d}$，不是特征向量，该方向的二阶导数是所有特征值的<code>加权平均</code>，与特征向量${\boldsymbol d}$的夹角越小的权重越大。最大特征值确定最大二阶导数，最小特征值确定最小二阶导数。</p>
<p>在多维的情况下，我们需要检测函数的所有二阶导数，可以用过检测Hessian的特征值来判断临界点是局部极大点、局部极小点还是鞍点。当Hessian矩阵<code>正定</code>时，临界点时局部极小点；<code>负定</code>时，是局部极大点。跟高中的导数和二阶导数是一样的。</p>
<p>用Hessian矩阵的信息来指导搜索，最简单的方法是牛顿法(Newton's method)。牛顿法基于一个二阶泰勒展开来近似${\boldsymbol x}^{(0)}$附近的$f(\boldsymbol x)$。当$f$是一个正定二次函数时，牛顿法只要用一次${\boldsymbol x}^*={\boldsymbol{x}}^{(0)}-{\boldsymbol{H}}(f)({\boldsymbol{x}}^{(0)})^{-1}\nabla_x f({\boldsymbol{x}}^{(0)})$就能直接条到函数的最小点。如果$f$不是一个真正二次但是能在局部近似为正定二次，牛顿法则需要迭代应用该式。<code>迭代地更新近似函数和跳到近似函数的最小点</code>可以比<code>梯度下降</code>更快地到达临界点。这在接近局部极小点时是一个特别有用的性质，但是<code>在鞍点附近是有害的</code>。</p>
<p>仅使用梯度信息的优化算法被称为<code>一阶优化算法</code>(first-order optimization algorithms)，如梯度下降。使用Hessian矩阵的优化算法被称为<code>二阶最优化算法</code>(second-order optimization algorithms)，如牛顿法。</p>
<blockquote>
<p><code>Lipschitz条件</code>即利普希茨连续条件，是一个比通常连续更强的<code>光滑性条件</code>。直觉上，Lipschitz连续函数限制了函数改变的速度，符合Lipschitz条件的函数的斜率，必小于一个称为Lipschitz常数的实数。</p>
</blockquote>
<p>在深度学习的背景下，限制函数满足Lipschitz连续或其导数Lipschitz连续可以获得一些保证。Lipschitz连续函数的变化速度以Lipschitz常数$\cal L$为界。这个属性允许我们量化我们的假设——梯度下降等算法导致的输入的微小变化将使输出只产生微小变化，因此是很有用的。（变化速度是有上界的）</p>
<p>最成功的特定优化领域或许是<code>凸优化</code>(Convex optimization)。凸优化通过<code>更强的限制</code>提供更多的保证。凸优化算法只对凸函数适用，即Hessian处处半正定的函数。因为这些函数没有鞍点而且其所有局部极小点必然是全局最小点。然而深度学习中的大多数问题都难以表示成凸优化的形式。凸优化仅用作一些深度学习算法的子程序。凸优化中的分析思路对证明深度学习算法的收敛性非常有用，然而一般来说，深度学习背景下凸优化的重要性大大减少。</p>
<ul>
<li>4.4 约束优化</li>
</ul>
<p>有时候，我们希望在$\boldsymbol{x}$所有可能值下最小化一个函数$f(x)$；有时候，我们可能希望在$\boldsymbol{x}$的某些集合$\Bbb{S}$中找$f(x)$最小值。这就是<code>约束优化</code>(constrained optimization)。在约束优化术语中，集合$\Bbb{S}$内的点$\boldsymbol{x}$被称为<code>可行点</code>(feasible)。我们尝尝希望找到在某种意义上小的解，针对这种情况下的常见方法是强加一个<code>范数约束</code>，如$\parallel{\boldsymbol{x}}\parallel\leq 1$。</p>
<p>约束优化的一个简单方法是将约束考虑在内后简单地对梯度下降进行修改。如果我们使用一个小的恒定的步长$\epsilon$，我们可以先取梯度下降的单步结果，然后将结果投影回$\Bbb{S}$。如果我们使用线搜索，我们只能在步长为$\epsilon$的范围内搜索可行的新$\boldsymbol{x}$点，或者我们可以将线上的每个点投影到约束区域。如果可能的话，在梯度下降或者线搜索前将梯度投影到可行域的切空间会更高效。</p>
<p>约束优化的一个更复杂的方法是设计一个不同的、无约束的优化问题，其解可以转化成原始约束问题的解。<code>Karush-Kuhn-Tucker(KKT)</code>方法是针对约束优化非常通用的解决方案。</p>
<p>介绍KKT方法，我们引入一个称为广义Lagrangian(generalized Lagrangian)或<code>广义Lagrange函数</code>(generalized Lagrange function)的新函数。为了定义Lagrangian，我们先要通过等式和不等式的形式描述$\Bbb{S}$。我们希望通过m个等式约束$g^{(i)}$和n个不等式约束$h^{(j)}$描述$\Bbb{S}$。我们为每个约束引入新的变量$\lambda_i$和$\alpha_j$，这些新变量被称为KKT乘子。广义的Lagrangian可以如下定义：</p>
<p>
<script type="math/tex; mode=display">L({\boldsymbol{x},\boldsymbol{\lambda},\boldsymbol{\alpha}})=f({\boldsymbol{x}})+\sum_i\lambda_i g^{(i)}({\boldsymbol{x}})+\sum_j\alpha_j h^{(j)}({\boldsymbol{x}})</script>
</p>
<p>现在可以通过优化无约束的广义Lagrangian解决约束最小化问题。只要存在至少一个可行点且$f(\boldsymbol{x})$不允许取$\infty$，那么</p>
<p>
<script type="math/tex; mode=display">\min_{\boldsymbol{x}} \max_{\boldsymbol{\lambda}} \max_{\boldsymbol{\alpha},\boldsymbol{\alpha}\geq0}L({\boldsymbol{x},\boldsymbol{\lambda},\boldsymbol{\alpha}})</script>
</p>
<p>与下列函数有相同的最优目标函数值和最优点集$\boldsymbol{x}$。</p>
<p>
<script type="math/tex; mode=display">\min_{\boldsymbol{x}\in\Bbb{S}}f({\boldsymbol{x}})</script>
</p>
<p>如果$h^{(i)}({\boldsymbol{x}}^*=0)$，则称这个不等式约束是<code>活跃</code>的。</p>
<p>我们可以使用一组简单的性质来描述约束优化问题的最优点。这些性质称为Karush-Kuhn-Tucker(KKT)条件。这些是确定一个点是最优点的必要条件，但不一定是充分条件。这些条件是：</p>
<p>(1)广义Lagrangian的梯度为0。</p>
<p>(2)所有关于$\boldsymbol{x}$和KKT乘子的约束都满足。</p>
<p>(3)不等式约束显示的“互补松弛性”：$\boldsymbol{\alpha}\odot h(\boldsymbol{x})=0$。</p>
<h1 id="_4">第五章</h1>
<p>机器学习本质上属于应用统计学，更多地关注如何用计算机统计地估计复杂函数，不太关注为这些函数提供置信区间；因此我们回探讨两种统计学地主要方法：<code>频率派估计</code>和<code>贝叶斯推断</code>。</p>
<ul>
<li>5.1 学习算法</li>
</ul>
<p>5.1.1 任务$T$</p>
<p>通畅机器学习<code>任务</code>定义为机器学习系统应该如何处理样本。<code>样本</code>是指我们从某些希望机器学习系统处理的对象或事件中收集到的已经量化的<code>特征</code>的集合。</p>
<p>机器学习可以解决很多类型的任务。一些非常常见的机器学习任务列举如下：</p>
<p>(1)分类：指定某些输入属于$k$类中的<code>哪一类</code>。还有输出不同类别的<code>概率分布</code>。分类任务中有一个任务是<code>对象识别</code>，输入是图片，输出是表示图片物体的数字码。</p>
<p>(2)输入缺失分类：（当输入向量的每个度量不被保证的时候）当一些输入可能丢失时，学习算法必须<code>学习一组函数</code>，而不是单个分类函数。每个函数对应着分类具有不同缺失输入子集的$\boldsymbol{x}$。（在医疗诊断中经常出现）有效地定义这样一个大集合函数的方法是学习所有相关变量的概率分布，然后通过边缘化缺失变量来解决分类任务。使用$n$个输入变量，我们现在可以获得每个可能的缺失输入集合所需的所有$2^n$个不同的分类函数，但是计算机程序仅需要学习一个描述联合概率分布的函数。</p>
<p>(3)回归：对给定输入<code>预测</code>数值。$f:\Bbb{R}^n\to\Bbb{R}$。</p>
<p>(4)转录：这类任务中，机器学习系统观测一些相对非结构化表示的数据，并转录信息为离散的文本形式。如<code>光学字符识别(OCR)</code>要求根据文本图片返回文字序列(ASKII码或Unicode码)。另一个例子是<code>语音识别</code>，输入一段音频波形，输出以序列音频纪录中所说的字符或单词ID的编码。</p>
<p>(5)机器翻译：输入是一种语言的符号序列，计算机程序将其转化成另一种语言的符号序列。（自然语言处理）</p>
<p>(6)结构化输出：结构化输出任务的输出是向量或者其他包含多个值的数据结构，并且构成输出的这些不同元素间具有重要关系。这是一个很大的范畴，包括上面的转录任务和翻译任务在内的很多其他任务。例如<code>语法分析</code>——映射自然语言句子到语法结构树，并<code>标记</code>树的节点为动词、名词、副词等等。另一个例子是图像的<code>像素级分割</code>，将每一个像素分配到特定类别。在这些标注型任务中，输出的结构形式不需要和输入尽可能相似。例如为图片添加描述的任务，输入图片，输出自然语言句子。这类任务被称为结构化输出任务是因为输出值之间内部紧密关系，例如添加的描述的单词必须组合成一个通顺的句子。</p>
<p>(7)异常检测：这类任务，计算机程序在一组事件或对象中筛选，并标记不正常或非典型的个体。例如引用卡欺诈检测，对人的购买习惯缄默，可以检测卡是否被滥用。</p>
<p>(8)合成和采样：这类任务中，机器学习程序生成一些和训练数据相似的新样本。合成和采样在媒体应用中非常有用，例如视频游戏可以<code>自动生成</code>大型物体或风景的纹理。某些情况下，我们希望采样或合成过程可以根据给定的输入生成一些<code>特定类型</code>的输出，例如在<code>语音合成</code>中，根据书写的句子输出句子语音的音频波形。这是一类结构化输出任务，但是多了每个输入并非只有一个正确输出的条件，并且我们希望输出有很多变化使得结果看上去更加自然和真实。</p>
<p>(9)缺失值填补：在这类任务中，机器学习算法对新样本$\boldsymbol{x}\in\Bbb{R}^n$中缺失的某些元素$x_i$进行填补。</p>
<p>(10)去噪：这类任务重，机器学习算法的输入是，干净样本$\boldsymbol{x}\in\Bbb{R}^n$经过未知损坏过程后得到的损坏样本$\tilde{\boldsymbol{x}}\in\Bbb{R}^n$。算法根据损坏后的样本$\tilde{\boldsymbol{x}}$预测干净的样本$\boldsymbol{x}$，或者更一般地预测条件概率分布$p(\boldsymbol{x}|\tilde{\boldsymbol{x}})$。</p>
<p>(11)密度估计或概率质量函数估计：在密度估计问题中，机器学习算法学习函数$p_{model}:\Bbb{R}^n\to\Bbb{R}$，其中$p_{model}(\boldsymbol{x})$可以解释为样本采样空间地概率密度函数（x连续）或者概率质量函数（x离散）。算法需要学习观测到的数据的结构。算法必须知道什么情况下样本聚集出现，什么情况下不太可能出现。以上描述的大多数任务都需要学习算法至少能隐式地捕获概率分布的结构。密度估计可以让我们<code>显式地捕获该分布</code>。原则上，我们可以在该分布上计算以便解决其他任务。例如可以用该分布解决缺失值填补任务。很多情况下概率分布$p(\boldsymbol{x})$是难以计算的。</p>
<p>5.1.2 性能度量$P$</p>
<p>评估机器学习算法的能力。</p>
<p>对于分类、缺失输入分类和转录任务，通常用准确率(accuracy)或错误率(error rate)。通常把错误率称为<code>0-1损失的期望</code>。</p>
<p>某些情况下很难确定应该度量什么，比如转录。还有一些情况，我们知道应该度量哪些数值，但是度量它们不太现实，比如密度估计。</p>
<p>5.1.3 经验$E$</p>
<p>均方误差(mean squared error)：</p>
<p>
<script type="math/tex; mode=display">MSE_{test}=\frac{1}{m}\sum_i({\hat{\boldsymbol{y}}}^{(test)}-\boldsymbol{y}^{(test)})</script>
</p>
<ul>
<li>5.2 容量、过拟合和欠拟合</li>
</ul>
<p>训练集上的度量误差称为训练误差(training error)。测试集上的误差叫做测试误差(test error)，也叫泛化误差(generalization error)。泛化误差被定义为新输入的<code>误差期望</code>。</p>
<p>训练时，我们职能观测到训练集，但是我们要的是在测试集上的性能。如果训练集和测试集是任意收集的，那么我们能做的很有限。如果我们可以对训练集和测试集数据的<code>收集方式</code>有些假设，那么我们能够对算法做些改进。</p>
<p>训练集和测试集通过数据集上被称为数据生成过程(data generating process)的概率分布生成。通常我们会做一系列被统称为<code>独立同分布假设</code>的假设。这个假设时说每个数据集中的样本都是彼此相互独立(independent)的，并且训练集和测试集是同分布的(identically distributed)，采样自相同的分布。这个假设我们能够在单个样本的概率分布描述数据生成过程。然后相同的分布可以用来生成每一个样本和每一个测试样本。我们将这个共享的潜在分布称为数据生成分布(data generating distribution)，记作$p_{data}$。这个概率框架和独立同分布假设允许我们从数学上研究训练误差和测试误差之间的关系。</p>
<p>决定机器学习算法效果是否好的因素：</p>
<p>(1)降低训练误差</p>
<p>(2)缩小训练误差和测试误差的差距。</p>
<p>上面这两个因素对应机器学习中的两个主要挑战：<code>欠拟合</code>(underfitting)和<code>过拟合</code>(overfitting)。欠拟合是指模型不能在训练集上获得足够低的误差。而过拟合是指训练误差和测试误差之间的差距太大。</p>
<p>通过调整模型的<code>容量</code>(capacity)，我们可以控制模型是否偏向于过拟合或者欠拟合。通俗地说，模型地容量是指<code>其拟合各种函数的能力</code>。容量低的模型可能很难拟合训练集。容量高的模型可能会过拟合，因为记住了不适用于测试集地训练集性质。</p>
<p>用于控制训练算法容量的方法是选择<code>假设空间</code>(hypothesis space)，即学习算法可以选择为解决方案的数据集。例如线性回归函数将关于其输入的所有线性函数作为假设空间。广义线性回归的假设空间包含多项式函数，而非仅有线性函数。这样做就增加了模型的容量。</p>
<p>算法的容量适合与任务的复杂度和所提供训练数据的数量时，算法效果通畅会最佳，容量不足不能解决复杂任务，容量高可能会过拟合。</p>
<p>还有很多方法可以改变模型的容量。容量不仅取决于模型的选择。模型规定了调整参数降低训练目标时，学习算法可以从哪些函数族中选择函数。这是模型的<code>表示容量</code>(representational capacity)。很多情况下，从这些函数中挑选出最优函数是非常困难的优化问题。实际中不会真的找到最优函数，而仅找到一个可以大大降低训练误差的函数。有一些<code>额外的限制因素</code>，比如优化算法的不完美，意味着学习算法的<code>有效容量</code>(effective capacity)可能小于模型族的表示容量。</p>
<p>提高模型泛化能力的一个简约原则，是<code>奥卡姆剃刀</code>(Occam's razor)，该原则是，在同样能够解释已知观测现象的假设中，应该挑选"最简单"的那一个。</p>
<p>统计学习理论提供了量化模型容量的不同方法。最有名的是Vapnik-Chervonenkis维度(Vapnik-Chervonenkis dimension, VC)。<code>VC维</code>度量二元分类器的容量。VC维定义为该分类器能够分类的训练样本的最大数目。假设存在$m$个不同$\boldsymbol{x}$点的训练集。分类器可以任意地标记该$m$个不同的$\boldsymbol{x}$，VC维被定义为$m$的最大可能值。</p>
<p>统计学理论中最重要的结论阐述了训练误差和泛化误差之间的差异的上界随着模型容量的增长而增长，随着训练样本增多而下降。这些边界很少用于深度学习中，一部分原因是边界太松，另一部分原因是<code>很难确定深度学习算法的容量</code>。有效容量受限于优化算法的能力，确定深度学习模型的容量特别困难。而且对于深度学习中的一般非凸优化问题，我们只有很少的理论分析。</p>
<p>线性回归是参数模型，学习到的函数在观测新数据欠，参数是有限且固定的向量。<code>非参数模型</code>(non-parametric)没有这样的限制。有时非参数模型仅是一些不能实际实现的理论抽象(比如搜索所有可能概率分布的算法)，然而我们也可以设计一些使用的非参数模型，使他们的复杂度和训练集大小有关。一个示例是最近邻回归(nearest neighbor regression)。</p>
<p>理想模型假设我们能够预先知道生成数据的真实概率分布。然而这样的模型仍然在很多问题上发生错误，因为分布中仍然会有一些噪声。从预先知道的真实分布$p(\boldsymbol{x},y)$预测而出现的误差被称为<code>贝叶斯误差</code>(Bayes error)。</p>
<p>5.2.1 没有免费的午餐定理</p>
<p>归纳推理，或是从一组有限的样本中推断一般的规则，在逻辑上不是很有效。在一定程度上，机器学习仅通过<code>概率法则</code>就可以避免这个问题，而无需使用纯逻辑推理整个确定性法则。机器学习保证找到一个在所关注的<code>大多数</code>样本上<code>可能</code>正确的规则。可惜，<code>没有免费午餐定理</code>(no free lunch theorem)表明，在所有可能的数据生成分布上平均之后，每一个分类算法在未实现观测的点上都有相同的错误率。换言之，在某种意义上，<code>没有一个机器学习算法总是比其他的要好</code>。我们能够设想的最先进的算法和简单地将所有点归为同一类地简单算法有着相同地平均性能(在所有可能地任务上)。</p>
<p>机器学习研究地目标不是找一个通用学习算法或是绝对最好地学习算法。反之，我们地目标是理解什么样地分布与人工智能获取经验地"真实世界"相关，什么样的学习算法<code>在我们关注的数据生成分布上效果最好</code>。</p>
<p>5.2.2 正则化</p>
<p>算法的效果不仅很大程度上受影响于<code>假设空间的函数数量</code>，也取决于这些<code>函数的具体形式</code>。我们可以通过两种方式控制算法的性能，(1)允许使用的函数种类；(2)这些函数的数量。</p>
<p>我们可以加入<code>权重衰减</code>(weight decay)来修改线性回归的训练标准。带权重衰减的线性回归最小化训练集上的<code>均方误差</code>和<code>正则项</code>的和$J(\boldsymbol{w})$，其偏好于平方范数$L^2$较小的权重。具体如下：</p>
<p>
<script type="math/tex; mode=display">J(\boldsymbol{w})=MSE_{train}+\lambda\boldsymbol{w}^T\boldsymbol{w}</script>
</p>
<p>更一般地，正则化一个学习函数$f(\boldsymbol{x};\boldsymbol{\theta})$，我们可以给代价函数添加被称为正则化项的惩罚。</p>
<p>表示对函数地偏好是比增减假设空间的成员函数更一般的控制模型容量的方法。有很多其他方法隐式或显式地表达对不同解地偏好。这些都是<code>正则化</code>(regularization)。正则化是指我们修改学习算法，使其降低泛化误差而非训练误差。正则化是机器学习领域地中心问题之一，只有优化能够与其重要性相媲。深度学习中普遍的(特别是本书)的理念是大量任务(例如人类能做的智能任务)也许都能使用非常通用的正则化形式来有效解决。</p>
<ul>
<li>5.3 超参数和验证集</li>
</ul>
<p>大多数机器学习算法都有<code>超参数</code>，可以设置来控制算法行为。超参数的值不是通过学习算法本身学习出来的。有时候一个选项被设为超参数，是因为它太难优化了。更多的情况是，该选项必须是超参数，因为它不适合在训练集上学习。</p>
<p>为了解决这个问题，我们需要一个训练算法观测不到的验证集(validtaion set)样本。<code>验证集是用来"训练"超参数的</code>。尽管验证集的误差通常会比训练集误差小，验证集会低估泛化误差。</p>
<p>5.3.1 交叉验证</p>
<p>小规模测试集意味着平均测试误差估计的统计不确定性，这使得很难啊判断算法A是否比算法B在给定的任务上做得更好。所以我们用k折交叉验证，代价是增加了计算量。</p>
<ul>
<li>5.4 估计、偏差和方差</li>
</ul>
<p>5.4.1 点估计</p>
<p><code>点估计</code>(point estimator)试图为一些感兴趣的量提供单个"最优"预测。一般地，感兴趣地量可以是<code>单个参数</code>，或是某些参数模型中的一个<code>向量参数</code>。为了区分参数估计和真实值，我们习惯将参数$\theta$的点估计表示为$\hat{\theta}$。</p>
<p>令$\{\boldsymbol{x}^{(1)},...,\boldsymbol{x}^{(m)}\}$是$m$个独立同分布的数据点。点估计或统计量(statistics)是这些数据的任意函数：</p>
<p>
<script type="math/tex; mode=display">\hat{\boldsymbol{\theta}}_m=g(\boldsymbol{x}^{(1)},...,\boldsymbol{x}^{(m)})</script>
</p>
<p>这个定义不要求$g$返回一个接近真实$\boldsymbol{\Theta}$的值，活着$g$的值域恰好是$\boldsymbol{\theta}$的允许取值范围。点估计的定义非常宽泛。</p>
<p>我们采取频率派在统计上的观点，假设真实参数$\boldsymbol{\theta}$是<code>固定但未知的</code>。</p>
<p>点估计也可以指输入和目标变量之间关系的估计。我们将这种类型的点估计称为<code>函数估计</code>。</p>
<p>有时候我们会关注函数估计(或函数近似)。这时我们试图从输入向量$\boldsymbol{x}$预测变量$\boldsymbol{y}$。我们假设有一个函数$f(\boldsymbol{x})$表示$\boldsymbol{y}$和$\boldsymbol{x}$之间的近似关系。在函数估计中，我们感兴趣的是用模型估计区近似$f$，或者估计$\hat{f}$。函数估计和参数估计$\boldsymbol{\theta}$是一样的，<code>函数估计</code>$\hat{f}$<code>是函数空间中的一个点估计</code>。</p>
<p>5.4.2 偏差</p>
<p>估计的<code>偏差</code>被定义为：</p>
<p>
<script type="math/tex; mode=display">bias(\boldsymbol{\hat{\theta}}_m)=\Bbb{E}(\boldsymbol{\hat{\theta}}_m)-\boldsymbol{\theta}</script>
</p>
<p>其中期望作用在所有数据(看作是从随机变量采样得到的)上，$\theta$是用于定义数据生成分布的$\theta$的真是值。如果$bias(\boldsymbol{\hat{\theta}}_m)=0$，则估计量$(\boldsymbol{\hat{\theta}}_m)$被称为是<code>无偏</code>(unbiased)。如果$\lim_{m\to\infty}bias(\boldsymbol{\hat{\theta}}_m)=0$，那么估计量$(\boldsymbol{\hat{\theta}}_m)$被称为是<code>渐进无偏</code>(asymptotically unbiased)。</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../section_2/" class="btn btn-neutral float-right" title="Section 2">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../Introduction/" class="btn btn-neutral" title="Introduction"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../Introduction/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../section_2/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script src="../.././mathjaxhelper.js"></script>
      <script src="../../search/require.js"></script>
      <script src="../../search/search.js"></script>

</body>
</html>
